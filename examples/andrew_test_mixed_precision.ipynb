{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ndsong/anaconda3/envs/awq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import gc\n",
    "from awq.quantize.quantizer import (\n",
    "    real_quantize_model_weight,\n",
    "    pseudo_quantize_model_weight,\n",
    "    pseudo_quantize_tensor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer):\n",
    "    testenc = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testenc[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    testenc = testenc.input_ids.to(model.device)\n",
    "    nsamples = 40\n",
    "    model = model.eval()\n",
    "\n",
    "    nlls = []\n",
    "    for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n",
    "        batch = testenc[:, (i * 2048) : ((i + 1) * 2048)].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            lm_logits = model(batch).logits\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "        shift_labels = testenc[:, (i * 2048) : ((i + 1) * 2048)][:, 1:]\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "        )\n",
    "        neg_log_likelihood = loss.float() * 2048\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))\n",
    "\n",
    "def get_model_size(model: nn.Module, data_width=16, group_size=-1):\n",
    "\n",
    "    if group_size != -1:\n",
    "        data_width += (16 + 4) / group_size\n",
    "\n",
    "    num_elements = 0\n",
    "    for param in model.parameters():\n",
    "        num_elements += param.numel()\n",
    "    return num_elements * data_width\n",
    "\n",
    "\n",
    "Byte = 8\n",
    "KiB = 1024 * Byte\n",
    "MiB = 1024 * KiB\n",
    "GiB = 1024 * MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_salient_weight_fp16(\n",
    "    model, w_bit, q_group_size, input_feat\n",
    "):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            importance = sum(input_feat[n]).float()\n",
    "\n",
    "            \"\"\"\n",
    "            for i in range(len(input_feat[n])):\n",
    "                print(\"input_feat[n][i].shape: {}\".format(input_feat[n][i].shape))\n",
    "            print(\"name = {}, importance.shape: {}\".format(n, importance.shape))\n",
    "            \"\"\"\n",
    "\n",
    "            ############### YOUR CODE STARTS HERE ###############\n",
    "\n",
    "            # Step 1: Find 1% of the salient weight channels according to importance (hint: use torch.topk())\n",
    "            outlier_indices = torch.topk(importance, k=int(importance.shape[0]/100), dim=0)[1]\n",
    "            assert outlier_indices.dim() == 1\n",
    "\n",
    "            ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "            # Back up the values of the salient weight channels\n",
    "            outlier = m.weight.data[:, outlier_indices].clone()\n",
    "\n",
    "            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n",
    "\n",
    "            ############### YOUR CODE STARTS HERE ###############\n",
    "\n",
    "            \n",
    "            # Step 2: Restore the 1% salient weight channels to their original FP16 values\n",
    "            m.weight.data[:, outlier_indices] = outlier\n",
    "\n",
    "            ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "def get_calib_dataset(tokenizer=None, n_samples=256, block_size=512):\n",
    "    dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    samples = []\n",
    "    n_run = 0\n",
    "    for data in dataset:\n",
    "        line = data[\"text\"]\n",
    "        line = line.strip()\n",
    "        line_encoded = tokenizer.encode(line)\n",
    "        if len(line_encoded) > block_size:\n",
    "            continue\n",
    "        sample = torch.tensor([line_encoded])\n",
    "        if sample.numel() == 0:\n",
    "            continue\n",
    "        samples.append(sample)\n",
    "        n_run += 1\n",
    "        if n_run == n_samples:\n",
    "            break\n",
    "\n",
    "    # now concatenate all samples and split according to block size\n",
    "    cat_samples = torch.cat(samples, dim=1)\n",
    "    n_split = cat_samples.shape[1] // block_size\n",
    "    print(f\" * Split into {n_split} blocks\")\n",
    "    return [cat_samples[:, i*block_size:(i+1)*block_size] for i in range(n_split)]\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_calib_feat(model, tokenizer):\n",
    "    input_dict = dict()\n",
    "    def stat_input_max_hook(m, x, y, name):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        x_max = x.view(-1, x.shape[-1]).abs().mean(dim=0).cpu().detach()\n",
    "        if name not in input_dict:\n",
    "            input_dict[name] = [x_max]\n",
    "        else:\n",
    "            input_dict[name] += [x_max]\n",
    "\n",
    "    hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            hooks.append(\n",
    "                m.register_forward_hook(\n",
    "                    partial(stat_input_max_hook, name=name)))\n",
    "\n",
    "    print(\"Collecting activation scales...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    samples = get_calib_dataset(tokenizer)\n",
    "    pbar = tqdm.tqdm(samples)\n",
    "    for input_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        model(input_ids)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    return input_dict\n",
    "\n",
    "# core quantization method (simulated quantization)\n",
    "def pseudo_quantize_tensor(w, n_bit=4, q_group_size=-1):\n",
    "    org_w_shape = w.shape\n",
    "    if q_group_size > 0:\n",
    "        assert org_w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "\n",
    "    assert w.dim() == 2\n",
    "\n",
    "    # Calculate the maximum (\\alpha) and minimum values (\\beta) in the tensor.\n",
    "    max_val = w.amax(dim=1, keepdim=True)\n",
    "    assert max_val.dim() == 2 and max_val.size(0) == w.size(0) and max_val.size(1) == 1\n",
    "    min_val = w.amin(dim=1, keepdim=True)\n",
    "    assert min_val.dim() == 2 and min_val.size(0) == w.size(0) and min_val.size(1) == 1\n",
    "\n",
    "    # Calculate the scale factor and zero point.  (Formula 1 & 2)\n",
    "    max_int = 2 ** n_bit - 1\n",
    "    scales = (max_val - min_val).clamp(min=1e-5) / max_int\n",
    "    assert scales.shape == max_val.shape\n",
    "    zeros = (-torch.round(min_val / scales)).clamp_(0, max_int)\n",
    "    assert scales.shape == min_val.shape\n",
    "\n",
    "    assert torch.isnan(scales).sum() == 0\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    # Quantize W: Map values in the range [\\beta, \\alpha] to lie within [0, 2^b - 1] (Formula 3)\n",
    "    w = torch.clamp(torch.round(w / scales) + zeros, 0, max_int)\n",
    "    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n",
    "\n",
    "    # Dequantize W (pseudo quantization, the inverse transformation of Formula 3)\n",
    "    w = (w - zeros) * scales\n",
    "    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n",
    "\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    w = w.reshape(org_w_shape)\n",
    "    return w\n",
    "\n",
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight(\n",
    "    model, w_bit, q_group_size,\n",
    "):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ndsong/anaconda3/envs/awq/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "evaluating...: 100%|██████████| 40/40 [00:20<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 14.46904\n",
      "model size: 2534.11728 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) FP16 model\n",
    "\n",
    "#model_path = \"facebook/opt-2.7b\"\n",
    "model_path = \"facebook/opt-1.3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "pseudo_quantize_model_weight(model, w_bit=16, q_group_size=128)\n",
    "\n",
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "model_size = get_model_size(model, data_width=16, group_size=128)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.5f}\")\n",
    "print(f\"model size: {model_size/MiB:.5f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ndsong/anaconda3/envs/awq/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activation scales...\n",
      " * Split into 127 blocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:13<00:00,  9.52it/s]\n",
      "evaluating...: 100%|██████████| 40/40 [00:20<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 14.78181\n",
      "model size: 651.91025 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) W4A16 but with salient weight protection\n",
    "\n",
    "# a) compute input features from calibration dataset\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "input_feat = get_calib_feat(model, tokenizer)\n",
    "\n",
    "# b) quantize the model weights to 4 bits while protecting the salient channels of those weights\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "pseudo_quantize_model_salient_weight_fp16(model, w_bit=4, q_group_size=128, input_feat=input_feat)\n",
    "\n",
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "model_size = get_model_size(model, data_width=4, group_size=128)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.5f}\")\n",
    "print(f\"model size: {model_size/MiB:.5f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from awq.quantize.quantizer import pseudo_quantize_tensor\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_token_absmax(t, n_bits=8):\n",
    "    # t.shape = (input seq_len, hidden_size)\n",
    "\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max(dim=-1, keepdim=True)[0]           # scales.shape = (input seq_len, 1) max along the channel dimension\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_tensor_absmax(t, n_bits=8):\n",
    "    print(\"t.shape: {}\".format(t.shape))\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max()                                  # scales.shape = (1) max along the entire tensor    \n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_token_absmax_salient(t, outlier_indices, n_bits=8):\n",
    "    # t.shape = (input seq_len, hidden_size)\n",
    "    # input_feats = list of tensors of shape (hidden_size,) <- my guess is that len(input_feats) = input seq_len\n",
    "\n",
    "    assert outlier_indices.dim() == 1           # shape = (1% of hidden_size,)\n",
    "    t_copy = t.clone()\n",
    "\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max(dim=-1, keepdim=True)[0]           # scales.shape = (input seq_len, 1) max along the channel dimension\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "\n",
    "    t[:, outlier_indices] = t_copy[:, outlier_indices]\n",
    "\n",
    "    return t\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_tensor_absmax_salient(t, outlier_indices, n_bits=8):\n",
    "    # t.shape = (input seq_len, hidden_size)\n",
    "    assert outlier_indices.dim() == 1           # shape = (1% of hidden_size,)\n",
    "    t_copy = t.clone()\n",
    "\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max()                                  # scales.shape = (1) max along the entire tensor    \n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "\n",
    "    t[:, outlier_indices] = t_copy[:, outlier_indices]\n",
    "\n",
    "    return t\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        w_n_bits=4,\n",
    "        a_n_bits=16,\n",
    "        act_quant: Literal[\"per_token\", \"per_tensor\", \"none\"] = \"per_token\",\n",
    "        quantize_output: bool = False,\n",
    "        outlier_indices: torch.Tensor = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randn(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.float16,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.zeros(\n",
    "                    (1, self.out_features), dtype=torch.float16, requires_grad=False\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "        if act_quant == \"per_token\":\n",
    "            self.act_quant_name = \"per_token\"\n",
    "            self.act_quant = partial(\n",
    "                quantize_activation_per_token_absmax, n_bits=a_n_bits\n",
    "            )\n",
    "        elif act_quant == \"per_tensor\":\n",
    "            self.act_quant_name = \"per_tensor\"\n",
    "            self.act_quant = partial(\n",
    "                quantize_activation_per_tensor_absmax, n_bits=a_n_bits\n",
    "            )\n",
    "        elif act_quant == \"per_token_salient\":\n",
    "            self.act_quant_name = \"per_token_salient\"\n",
    "            self.act_quant = partial(\n",
    "                quantize_activation_per_token_absmax_salient, \n",
    "                outlier_indices=outlier_indices,\n",
    "                n_bits=a_n_bits,\n",
    "            )\n",
    "        elif act_quant == \"per_tensor_salient\":\n",
    "            self.act_quant_name = \"per_tensor_salient\"\n",
    "            self.act_quant = partial(\n",
    "                quantize_activation_per_tensor_absmax_salient, \n",
    "                outlier_indices=outlier_indices,\n",
    "                n_bits=a_n_bits,\n",
    "            )\n",
    "        else:\n",
    "            self.act_quant_name = \"None\"\n",
    "            self.act_quant = lambda x: x\n",
    "\n",
    "        if quantize_output:\n",
    "            self.output_quant_name = self.act_quant_name\n",
    "            self.output_quant = self.act_quant\n",
    "        else:\n",
    "            self.output_quant_name = \"None\"\n",
    "            self.output_quant = lambda x: x\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super(QuantizedLinear, self).to(*args, **kwargs)\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        q_x = self.act_quant(x)\n",
    "        y = F.linear(q_x, self.weight, self.bias)\n",
    "        q_y = self.output_quant(y)\n",
    "        return q_y\n",
    "\n",
    "    @classmethod\n",
    "    def from_linear(\n",
    "        cls,\n",
    "        linear: nn.Linear,\n",
    "        w_n_bits: int = 4,\n",
    "        a_n_bits: int = 4,\n",
    "        zero_point: bool = True,\n",
    "        group_size: int = 128,\n",
    "        act_quant: Literal[\"per_token\", \"per_tensor\", \"none\"] = \"per_token\",\n",
    "        quantize_output: bool = False,\n",
    "    ):\n",
    "\n",
    "        # this is a linear layer that will eventually enhouse the quantized weights\n",
    "        awq_linear = cls(\n",
    "            linear.in_features,\n",
    "            linear.out_features,\n",
    "            bias=linear.bias is not None,\n",
    "            w_n_bits=w_n_bits,\n",
    "            a_n_bits=a_n_bits,  \n",
    "            act_quant=act_quant,\n",
    "            quantize_output=quantize_output,\n",
    "        )\n",
    "\n",
    "        awq_linear.weight.data = pseudo_quantize_tensor(\n",
    "            w=linear.weight.data,\n",
    "            n_bit=w_n_bits,\n",
    "            zero_point=zero_point,\n",
    "            q_group_size=group_size,\n",
    "        )\n",
    "\n",
    "        if linear.bias is not None:\n",
    "            awq_linear.bias.data = linear.bias.data\n",
    "\n",
    "        return awq_linear\n",
    "\n",
    "    @classmethod\n",
    "    def from_linear_salient_weight(\n",
    "        cls,\n",
    "        linear: nn.Linear,\n",
    "        input_feats: torch.Tensor,\n",
    "        w_n_bits: int = 4,\n",
    "        a_n_bits: int = 4,\n",
    "        zero_point: bool = True,\n",
    "        group_size: int = 128,\n",
    "        act_quant: Literal[\"per_token\", \"per_tensor\", \"none\"] = \"per_token\",\n",
    "        quantize_output: bool = False,\n",
    "    ):\n",
    "\n",
    "        # this is a linear layer that will eventually enhouse the quantized weights\n",
    "        awq_linear = cls(\n",
    "            linear.in_features,\n",
    "            linear.out_features,\n",
    "            bias=linear.bias is not None,\n",
    "            w_n_bits=w_n_bits,\n",
    "            a_n_bits=a_n_bits,\n",
    "            act_quant=act_quant,\n",
    "            quantize_output=quantize_output,\n",
    "        )\n",
    "\n",
    "        awq_linear.weight.data = pseudo_quantize_tensor(\n",
    "            w=linear.weight.data,\n",
    "            n_bit=w_n_bits,\n",
    "            zero_point=zero_point,\n",
    "            q_group_size=group_size,\n",
    "        ) \n",
    "\n",
    "        # Step 1: Find 1% of the salient weight channels according to importance (hint: use torch.topk())\n",
    "        importance = sum(input_feats).float()\n",
    "        outlier_indices = torch.topk(importance, k=int(importance.shape[0]/100), dim=0)[1]\n",
    "        assert outlier_indices.dim() == 1\n",
    "            \n",
    "        # Step 2: Restore the 1% salient weight channels to their original FP16 values\n",
    "        outlier = linear.weight.data[:, outlier_indices].clone()\n",
    "        awq_linear.weight.data[:, outlier_indices] = outlier\n",
    "\n",
    "        if linear.bias is not None:\n",
    "            awq_linear.bias.data = linear.bias.data\n",
    "\n",
    "        return awq_linear\n",
    "\n",
    "    @classmethod\n",
    "    def from_linear_salient_weight_act(\n",
    "        cls,\n",
    "        linear: nn.Linear,\n",
    "        input_feats: torch.Tensor,\n",
    "        w_n_bits: int = 4,\n",
    "        a_n_bits: int = 4,\n",
    "        zero_point: bool = True,\n",
    "        group_size: int = 128,\n",
    "        act_quant: Literal[\"per_token\", \"per_tensor\", \"none\"] = \"per_token\",\n",
    "        quantize_output: bool = False,\n",
    "    ):\n",
    "\n",
    "        # Step 1: Find 1% of the salient weight channels according to importance (hint: use torch.topk())\n",
    "        importance = sum(input_feats).float()\n",
    "        outlier_indices = torch.topk(importance, k=int(importance.shape[0]/100), dim=0)[1]\n",
    "        assert outlier_indices.dim() == 1\n",
    "\n",
    "        # this is a linear layer that will eventually enhouse the quantized weights\n",
    "        awq_linear = cls(\n",
    "            linear.in_features,\n",
    "            linear.out_features,\n",
    "            bias=linear.bias is not None,\n",
    "            w_n_bits=w_n_bits,\n",
    "            a_n_bits=a_n_bits,\n",
    "            act_quant=act_quant + \"_salient\",\n",
    "            quantize_output = quantize_output,\n",
    "            outlier_indices = outlier_indices,\n",
    "        )\n",
    "\n",
    "        awq_linear.weight.data = pseudo_quantize_tensor(\n",
    "            w=linear.weight.data,\n",
    "            n_bit=w_n_bits,\n",
    "            zero_point=zero_point,\n",
    "            q_group_size=group_size,\n",
    "        )\n",
    "            \n",
    "        # Step 2: Restore the 1% salient weight channels to their original FP16 values\n",
    "        outlier = linear.weight.data[:, outlier_indices].clone()\n",
    "        awq_linear.weight.data[:, outlier_indices] = outlier\n",
    "\n",
    "        if linear.bias is not None:\n",
    "            awq_linear.bias.data = linear.bias.data\n",
    "\n",
    "        return awq_linear\n",
    "\n",
    "def quantize_opt(\n",
    "    model,\n",
    "    w_n_bits: int = 4,\n",
    "    a_n_bits: int = 4,\n",
    "    zero_point: bool = True,\n",
    "    group_size: int = 128,\n",
    "    act_quant: Literal[\"per_token\", \"per_tensor\", \"none\"] = \"per_token\",\n",
    "    quantize_bmm_input: bool = True,\n",
    "):\n",
    "    from transformers.models.opt.modeling_opt import (\n",
    "        OPTAttention,\n",
    "        OPTDecoderLayer,\n",
    "    )\n",
    "\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, OPTDecoderLayer):\n",
    "            m.fc1 = QuantizedLinear.from_linear(\n",
    "                m.fc1,\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "            )\n",
    "            m.fc2 = QuantizedLinear.from_linear(\n",
    "                m.fc2,\n",
    "                w_n_bits=w_n_bits,          # new input param\n",
    "                a_n_bits=a_n_bits,          # new input param\n",
    "                zero_point=zero_point,      # new input param\n",
    "                group_size=group_size,      # new input param\n",
    "                act_quant=act_quant,        # new input param\n",
    "            )\n",
    "        elif isinstance(m, OPTAttention):\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            m.q_proj = QuantizedLinear.from_linear(\n",
    "                m.q_proj,\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.k_proj = QuantizedLinear.from_linear(\n",
    "                m.k_proj,\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.v_proj = QuantizedLinear.from_linear(\n",
    "                m.v_proj,\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.out_proj = QuantizedLinear.from_linear(\n",
    "                m.out_proj,\n",
    "                w_n_bits=w_n_bits,          # new input param\n",
    "                a_n_bits=a_n_bits,          # new input param\n",
    "                zero_point=zero_point,      # new input param\n",
    "                group_size=group_size,      # new input param\n",
    "                act_quant=act_quant,        # new input param\n",
    "            )\n",
    "\n",
    "    return model\n",
    "\n",
    "def quantize_opt_salient_weight_fp16(\n",
    "    model,\n",
    "    input_feats,\n",
    "    w_n_bits: int = 4,\n",
    "    a_n_bits: int = 4,\n",
    "    zero_point: bool = True,\n",
    "    group_size: int = 128,\n",
    "    act_quant: Literal[\"per_token\", \"per_tensor\", \"none\"] = \"per_token\",\n",
    "    quantize_bmm_input: bool = True,\n",
    "):\n",
    "    from transformers.models.opt.modeling_opt import (\n",
    "        OPTAttention,\n",
    "        OPTDecoderLayer,\n",
    "    )\n",
    "\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, OPTDecoderLayer):\n",
    "            m.fc1 = QuantizedLinear.from_linear_salient_weight(\n",
    "                m.fc1,\n",
    "                input_feats[\"model.\" + name + \".fc1\"],\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "            )\n",
    "            m.fc2 = QuantizedLinear.from_linear_salient_weight(\n",
    "                m.fc2,\n",
    "                input_feats[\"model.\" + name + \".fc2\"],\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "            )\n",
    "        elif isinstance(m, OPTAttention):\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            m.q_proj = QuantizedLinear.from_linear_salient_weight(\n",
    "                m.q_proj,\n",
    "                input_feats[\"model.\" + name + \".q_proj\"],\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.k_proj = QuantizedLinear.from_linear_salient_weight(\n",
    "                m.k_proj,\n",
    "                input_feats[\"model.\" + name + \".k_proj\"],\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.v_proj = QuantizedLinear.from_linear_salient_weight(\n",
    "                m.v_proj,\n",
    "                input_feats[\"model.\" + name + \".v_proj\"],\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.out_proj = QuantizedLinear.from_linear_salient_weight(\n",
    "                m.out_proj,\n",
    "                input_feats[\"model.\" + name + \".out_proj\"],\n",
    "                w_n_bits=w_n_bits,          # new input param\n",
    "                a_n_bits=a_n_bits,          # new input param\n",
    "                zero_point=zero_point,      # new input param\n",
    "                group_size=group_size,      # new input param\n",
    "                act_quant=act_quant,        # new input param\n",
    "            )\n",
    "\n",
    "    return model\n",
    "\n",
    "def quantize_opt_salient_weight_act_fp16(\n",
    "    model,\n",
    "    input_feats,\n",
    "    w_n_bits: int = 4,\n",
    "    a_n_bits: int = 4,\n",
    "    zero_point: bool = True,\n",
    "    group_size: int = 128,\n",
    "    act_quant: Literal[\"per_token\", \"per_tensor\", \"none\"] = \"per_token\",\n",
    "    quantize_bmm_input: bool = True,\n",
    "):\n",
    "    from transformers.models.opt.modeling_opt import (\n",
    "        OPTAttention,\n",
    "        OPTDecoderLayer,\n",
    "    )\n",
    "\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, OPTDecoderLayer):\n",
    "            m.fc1 = QuantizedLinear.from_linear_salient_weight_act(\n",
    "                m.fc1,\n",
    "                input_feats[\"model.\" + name + \".fc1\"],\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "            )\n",
    "            m.fc2 = QuantizedLinear.from_linear_salient_weight_act(\n",
    "                m.fc2,\n",
    "                input_feats[\"model.\" + name + \".fc2\"],\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "            )\n",
    "        elif isinstance(m, OPTAttention):\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            m.q_proj = QuantizedLinear.from_linear_salient_weight_act(\n",
    "                m.q_proj,\n",
    "                input_feats[\"model.\" + name + \".q_proj\"],\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.k_proj = QuantizedLinear.from_linear_salient_weight_act(\n",
    "                m.k_proj,\n",
    "                input_feats[\"model.\" + name + \".k_proj\"],\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.v_proj = QuantizedLinear.from_linear_salient_weight_act(\n",
    "                m.v_proj,\n",
    "                input_feats[\"model.\" + name + \".v_proj\"],\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "            )\n",
    "            m.out_proj = QuantizedLinear.from_linear_salient_weight_act(\n",
    "                m.out_proj,\n",
    "                input_feats[\"model.\" + name + \".out_proj\"],\n",
    "                w_n_bits=w_n_bits,          # new input param\n",
    "                a_n_bits=a_n_bits,          # new input param\n",
    "                zero_point=zero_point,      # new input param\n",
    "                group_size=group_size,      # new input param\n",
    "                act_quant=act_quant,        # new input param\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 14.93691\n",
      "model size: 104.38248 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# naive W8A8 quantization (FP 16 model = 14.46904 perplexity for opt1.3b and 12.35642 perplexity for opt2.7b)\n",
    "\n",
    "w_n_bits = 8\n",
    "a_n_bits = 8\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "model = quantize_opt(model, w_n_bits=w_n_bits, a_n_bits=a_n_bits, act_quant=\"per_token\")              # perplexity = 14.55xxx\n",
    "model.cuda()\n",
    "\n",
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "model_size = get_model_size(model, data_width=w_n_bits, group_size=128)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.5f}\")\n",
    "print(f\"model size: {model_size/MiB:.5f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activation scales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Split into 127 blocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:13<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model.decoder.layers.0.self_attn.q_proj', 'model.decoder.layers.0.self_attn.k_proj', 'model.decoder.layers.0.self_attn.v_proj', 'model.decoder.layers.0.self_attn.out_proj', 'model.decoder.layers.0.fc1', 'model.decoder.layers.0.fc2', 'model.decoder.layers.1.self_attn.q_proj', 'model.decoder.layers.1.self_attn.k_proj', 'model.decoder.layers.1.self_attn.v_proj', 'model.decoder.layers.1.self_attn.out_proj', 'model.decoder.layers.1.fc1', 'model.decoder.layers.1.fc2', 'model.decoder.layers.2.self_attn.q_proj', 'model.decoder.layers.2.self_attn.k_proj', 'model.decoder.layers.2.self_attn.v_proj', 'model.decoder.layers.2.self_attn.out_proj', 'model.decoder.layers.2.fc1', 'model.decoder.layers.2.fc2', 'model.decoder.layers.3.self_attn.q_proj', 'model.decoder.layers.3.self_attn.k_proj', 'model.decoder.layers.3.self_attn.v_proj', 'model.decoder.layers.3.self_attn.out_proj', 'model.decoder.layers.3.fc1', 'model.decoder.layers.3.fc2', 'model.decoder.layers.4.self_attn.q_proj', 'model.decoder.layers.4.self_attn.k_proj', 'model.decoder.layers.4.self_attn.v_proj', 'model.decoder.layers.4.self_attn.out_proj', 'model.decoder.layers.4.fc1', 'model.decoder.layers.4.fc2', 'model.decoder.layers.5.self_attn.q_proj', 'model.decoder.layers.5.self_attn.k_proj', 'model.decoder.layers.5.self_attn.v_proj', 'model.decoder.layers.5.self_attn.out_proj', 'model.decoder.layers.5.fc1', 'model.decoder.layers.5.fc2', 'model.decoder.layers.6.self_attn.q_proj', 'model.decoder.layers.6.self_attn.k_proj', 'model.decoder.layers.6.self_attn.v_proj', 'model.decoder.layers.6.self_attn.out_proj', 'model.decoder.layers.6.fc1', 'model.decoder.layers.6.fc2', 'model.decoder.layers.7.self_attn.q_proj', 'model.decoder.layers.7.self_attn.k_proj', 'model.decoder.layers.7.self_attn.v_proj', 'model.decoder.layers.7.self_attn.out_proj', 'model.decoder.layers.7.fc1', 'model.decoder.layers.7.fc2', 'model.decoder.layers.8.self_attn.q_proj', 'model.decoder.layers.8.self_attn.k_proj', 'model.decoder.layers.8.self_attn.v_proj', 'model.decoder.layers.8.self_attn.out_proj', 'model.decoder.layers.8.fc1', 'model.decoder.layers.8.fc2', 'model.decoder.layers.9.self_attn.q_proj', 'model.decoder.layers.9.self_attn.k_proj', 'model.decoder.layers.9.self_attn.v_proj', 'model.decoder.layers.9.self_attn.out_proj', 'model.decoder.layers.9.fc1', 'model.decoder.layers.9.fc2', 'model.decoder.layers.10.self_attn.q_proj', 'model.decoder.layers.10.self_attn.k_proj', 'model.decoder.layers.10.self_attn.v_proj', 'model.decoder.layers.10.self_attn.out_proj', 'model.decoder.layers.10.fc1', 'model.decoder.layers.10.fc2', 'model.decoder.layers.11.self_attn.q_proj', 'model.decoder.layers.11.self_attn.k_proj', 'model.decoder.layers.11.self_attn.v_proj', 'model.decoder.layers.11.self_attn.out_proj', 'model.decoder.layers.11.fc1', 'model.decoder.layers.11.fc2', 'model.decoder.layers.12.self_attn.q_proj', 'model.decoder.layers.12.self_attn.k_proj', 'model.decoder.layers.12.self_attn.v_proj', 'model.decoder.layers.12.self_attn.out_proj', 'model.decoder.layers.12.fc1', 'model.decoder.layers.12.fc2', 'model.decoder.layers.13.self_attn.q_proj', 'model.decoder.layers.13.self_attn.k_proj', 'model.decoder.layers.13.self_attn.v_proj', 'model.decoder.layers.13.self_attn.out_proj', 'model.decoder.layers.13.fc1', 'model.decoder.layers.13.fc2', 'model.decoder.layers.14.self_attn.q_proj', 'model.decoder.layers.14.self_attn.k_proj', 'model.decoder.layers.14.self_attn.v_proj', 'model.decoder.layers.14.self_attn.out_proj', 'model.decoder.layers.14.fc1', 'model.decoder.layers.14.fc2', 'model.decoder.layers.15.self_attn.q_proj', 'model.decoder.layers.15.self_attn.k_proj', 'model.decoder.layers.15.self_attn.v_proj', 'model.decoder.layers.15.self_attn.out_proj', 'model.decoder.layers.15.fc1', 'model.decoder.layers.15.fc2', 'model.decoder.layers.16.self_attn.q_proj', 'model.decoder.layers.16.self_attn.k_proj', 'model.decoder.layers.16.self_attn.v_proj', 'model.decoder.layers.16.self_attn.out_proj', 'model.decoder.layers.16.fc1', 'model.decoder.layers.16.fc2', 'model.decoder.layers.17.self_attn.q_proj', 'model.decoder.layers.17.self_attn.k_proj', 'model.decoder.layers.17.self_attn.v_proj', 'model.decoder.layers.17.self_attn.out_proj', 'model.decoder.layers.17.fc1', 'model.decoder.layers.17.fc2', 'model.decoder.layers.18.self_attn.q_proj', 'model.decoder.layers.18.self_attn.k_proj', 'model.decoder.layers.18.self_attn.v_proj', 'model.decoder.layers.18.self_attn.out_proj', 'model.decoder.layers.18.fc1', 'model.decoder.layers.18.fc2', 'model.decoder.layers.19.self_attn.q_proj', 'model.decoder.layers.19.self_attn.k_proj', 'model.decoder.layers.19.self_attn.v_proj', 'model.decoder.layers.19.self_attn.out_proj', 'model.decoder.layers.19.fc1', 'model.decoder.layers.19.fc2', 'model.decoder.layers.20.self_attn.q_proj', 'model.decoder.layers.20.self_attn.k_proj', 'model.decoder.layers.20.self_attn.v_proj', 'model.decoder.layers.20.self_attn.out_proj', 'model.decoder.layers.20.fc1', 'model.decoder.layers.20.fc2', 'model.decoder.layers.21.self_attn.q_proj', 'model.decoder.layers.21.self_attn.k_proj', 'model.decoder.layers.21.self_attn.v_proj', 'model.decoder.layers.21.self_attn.out_proj', 'model.decoder.layers.21.fc1', 'model.decoder.layers.21.fc2', 'model.decoder.layers.22.self_attn.q_proj', 'model.decoder.layers.22.self_attn.k_proj', 'model.decoder.layers.22.self_attn.v_proj', 'model.decoder.layers.22.self_attn.out_proj', 'model.decoder.layers.22.fc1', 'model.decoder.layers.22.fc2', 'model.decoder.layers.23.self_attn.q_proj', 'model.decoder.layers.23.self_attn.k_proj', 'model.decoder.layers.23.self_attn.v_proj', 'model.decoder.layers.23.self_attn.out_proj', 'model.decoder.layers.23.fc1', 'model.decoder.layers.23.fc2', 'lm_head'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 14.91101\n",
      "model size: 104.38248 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# W8A8 quantization with salient weight protection (FP 16 model = 14.46904 perplexity for opt1.3b and 12.35642 perplexity for opt2.7b)\n",
    "\n",
    "w_n_bits = 8\n",
    "a_n_bits = 8\n",
    "\n",
    "# a) compute input features from calibration dataset\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "input_feats = get_calib_feat(model, tokenizer)\n",
    "\n",
    "# print out all keys in input_feats\n",
    "print(input_feats.keys())\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "model = quantize_opt_salient_weight_fp16(model, input_feats, w_n_bits=w_n_bits, a_n_bits=a_n_bits, act_quant=\"per_token\")\n",
    "#model = quantize_opt_salient_weight_fp16(model, input_feats, w_n_bits=w_n_bits, a_n_bits=a_n_bits, act_quant=\"per_channel\")\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "model_size = get_model_size(model, data_width=w_n_bits, group_size=128)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.5f}\")\n",
    "print(f\"model size: {model_size/MiB:.5f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activation scales...\n",
      " * Split into 127 blocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:13<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model.decoder.layers.0.self_attn.q_proj', 'model.decoder.layers.0.self_attn.k_proj', 'model.decoder.layers.0.self_attn.v_proj', 'model.decoder.layers.0.self_attn.out_proj', 'model.decoder.layers.0.fc1', 'model.decoder.layers.0.fc2', 'model.decoder.layers.1.self_attn.q_proj', 'model.decoder.layers.1.self_attn.k_proj', 'model.decoder.layers.1.self_attn.v_proj', 'model.decoder.layers.1.self_attn.out_proj', 'model.decoder.layers.1.fc1', 'model.decoder.layers.1.fc2', 'model.decoder.layers.2.self_attn.q_proj', 'model.decoder.layers.2.self_attn.k_proj', 'model.decoder.layers.2.self_attn.v_proj', 'model.decoder.layers.2.self_attn.out_proj', 'model.decoder.layers.2.fc1', 'model.decoder.layers.2.fc2', 'model.decoder.layers.3.self_attn.q_proj', 'model.decoder.layers.3.self_attn.k_proj', 'model.decoder.layers.3.self_attn.v_proj', 'model.decoder.layers.3.self_attn.out_proj', 'model.decoder.layers.3.fc1', 'model.decoder.layers.3.fc2', 'model.decoder.layers.4.self_attn.q_proj', 'model.decoder.layers.4.self_attn.k_proj', 'model.decoder.layers.4.self_attn.v_proj', 'model.decoder.layers.4.self_attn.out_proj', 'model.decoder.layers.4.fc1', 'model.decoder.layers.4.fc2', 'model.decoder.layers.5.self_attn.q_proj', 'model.decoder.layers.5.self_attn.k_proj', 'model.decoder.layers.5.self_attn.v_proj', 'model.decoder.layers.5.self_attn.out_proj', 'model.decoder.layers.5.fc1', 'model.decoder.layers.5.fc2', 'model.decoder.layers.6.self_attn.q_proj', 'model.decoder.layers.6.self_attn.k_proj', 'model.decoder.layers.6.self_attn.v_proj', 'model.decoder.layers.6.self_attn.out_proj', 'model.decoder.layers.6.fc1', 'model.decoder.layers.6.fc2', 'model.decoder.layers.7.self_attn.q_proj', 'model.decoder.layers.7.self_attn.k_proj', 'model.decoder.layers.7.self_attn.v_proj', 'model.decoder.layers.7.self_attn.out_proj', 'model.decoder.layers.7.fc1', 'model.decoder.layers.7.fc2', 'model.decoder.layers.8.self_attn.q_proj', 'model.decoder.layers.8.self_attn.k_proj', 'model.decoder.layers.8.self_attn.v_proj', 'model.decoder.layers.8.self_attn.out_proj', 'model.decoder.layers.8.fc1', 'model.decoder.layers.8.fc2', 'model.decoder.layers.9.self_attn.q_proj', 'model.decoder.layers.9.self_attn.k_proj', 'model.decoder.layers.9.self_attn.v_proj', 'model.decoder.layers.9.self_attn.out_proj', 'model.decoder.layers.9.fc1', 'model.decoder.layers.9.fc2', 'model.decoder.layers.10.self_attn.q_proj', 'model.decoder.layers.10.self_attn.k_proj', 'model.decoder.layers.10.self_attn.v_proj', 'model.decoder.layers.10.self_attn.out_proj', 'model.decoder.layers.10.fc1', 'model.decoder.layers.10.fc2', 'model.decoder.layers.11.self_attn.q_proj', 'model.decoder.layers.11.self_attn.k_proj', 'model.decoder.layers.11.self_attn.v_proj', 'model.decoder.layers.11.self_attn.out_proj', 'model.decoder.layers.11.fc1', 'model.decoder.layers.11.fc2', 'model.decoder.layers.12.self_attn.q_proj', 'model.decoder.layers.12.self_attn.k_proj', 'model.decoder.layers.12.self_attn.v_proj', 'model.decoder.layers.12.self_attn.out_proj', 'model.decoder.layers.12.fc1', 'model.decoder.layers.12.fc2', 'model.decoder.layers.13.self_attn.q_proj', 'model.decoder.layers.13.self_attn.k_proj', 'model.decoder.layers.13.self_attn.v_proj', 'model.decoder.layers.13.self_attn.out_proj', 'model.decoder.layers.13.fc1', 'model.decoder.layers.13.fc2', 'model.decoder.layers.14.self_attn.q_proj', 'model.decoder.layers.14.self_attn.k_proj', 'model.decoder.layers.14.self_attn.v_proj', 'model.decoder.layers.14.self_attn.out_proj', 'model.decoder.layers.14.fc1', 'model.decoder.layers.14.fc2', 'model.decoder.layers.15.self_attn.q_proj', 'model.decoder.layers.15.self_attn.k_proj', 'model.decoder.layers.15.self_attn.v_proj', 'model.decoder.layers.15.self_attn.out_proj', 'model.decoder.layers.15.fc1', 'model.decoder.layers.15.fc2', 'model.decoder.layers.16.self_attn.q_proj', 'model.decoder.layers.16.self_attn.k_proj', 'model.decoder.layers.16.self_attn.v_proj', 'model.decoder.layers.16.self_attn.out_proj', 'model.decoder.layers.16.fc1', 'model.decoder.layers.16.fc2', 'model.decoder.layers.17.self_attn.q_proj', 'model.decoder.layers.17.self_attn.k_proj', 'model.decoder.layers.17.self_attn.v_proj', 'model.decoder.layers.17.self_attn.out_proj', 'model.decoder.layers.17.fc1', 'model.decoder.layers.17.fc2', 'model.decoder.layers.18.self_attn.q_proj', 'model.decoder.layers.18.self_attn.k_proj', 'model.decoder.layers.18.self_attn.v_proj', 'model.decoder.layers.18.self_attn.out_proj', 'model.decoder.layers.18.fc1', 'model.decoder.layers.18.fc2', 'model.decoder.layers.19.self_attn.q_proj', 'model.decoder.layers.19.self_attn.k_proj', 'model.decoder.layers.19.self_attn.v_proj', 'model.decoder.layers.19.self_attn.out_proj', 'model.decoder.layers.19.fc1', 'model.decoder.layers.19.fc2', 'model.decoder.layers.20.self_attn.q_proj', 'model.decoder.layers.20.self_attn.k_proj', 'model.decoder.layers.20.self_attn.v_proj', 'model.decoder.layers.20.self_attn.out_proj', 'model.decoder.layers.20.fc1', 'model.decoder.layers.20.fc2', 'model.decoder.layers.21.self_attn.q_proj', 'model.decoder.layers.21.self_attn.k_proj', 'model.decoder.layers.21.self_attn.v_proj', 'model.decoder.layers.21.self_attn.out_proj', 'model.decoder.layers.21.fc1', 'model.decoder.layers.21.fc2', 'model.decoder.layers.22.self_attn.q_proj', 'model.decoder.layers.22.self_attn.k_proj', 'model.decoder.layers.22.self_attn.v_proj', 'model.decoder.layers.22.self_attn.out_proj', 'model.decoder.layers.22.fc1', 'model.decoder.layers.22.fc2', 'model.decoder.layers.23.self_attn.q_proj', 'model.decoder.layers.23.self_attn.k_proj', 'model.decoder.layers.23.self_attn.v_proj', 'model.decoder.layers.23.self_attn.out_proj', 'model.decoder.layers.23.fc1', 'model.decoder.layers.23.fc2', 'lm_head'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 14.68109\n",
      "model size: 53.19107 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# W4A16 quantization with salient weight protection (reference implementation from homework = 14.78181)\n",
    "w_n_bits = 4\n",
    "a_n_bits = 16\n",
    "\n",
    "# a) compute input features from calibration dataset\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "input_feats = get_calib_feat(model, tokenizer)\n",
    "\n",
    "# print out all keys in input_feats\n",
    "print(input_feats.keys())\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "model = quantize_opt_salient_weight_fp16(model, input_feats, w_n_bits=w_n_bits, a_n_bits=a_n_bits, act_quant=\"per_token\")\n",
    "#model = quantize_opt_salient_weight_fp16(model, input_feats, w_n_bits=w_n_bits, a_n_bits=a_n_bits, act_quant=\"per_channel\")\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "model_size = get_model_size(model, data_width=w_n_bits, group_size=128)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.5f}\")\n",
    "print(f\"model size: {model_size/MiB:.5f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activation scales...\n",
      " * Split into 127 blocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:13<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model.decoder.layers.0.self_attn.q_proj', 'model.decoder.layers.0.self_attn.k_proj', 'model.decoder.layers.0.self_attn.v_proj', 'model.decoder.layers.0.self_attn.out_proj', 'model.decoder.layers.0.fc1', 'model.decoder.layers.0.fc2', 'model.decoder.layers.1.self_attn.q_proj', 'model.decoder.layers.1.self_attn.k_proj', 'model.decoder.layers.1.self_attn.v_proj', 'model.decoder.layers.1.self_attn.out_proj', 'model.decoder.layers.1.fc1', 'model.decoder.layers.1.fc2', 'model.decoder.layers.2.self_attn.q_proj', 'model.decoder.layers.2.self_attn.k_proj', 'model.decoder.layers.2.self_attn.v_proj', 'model.decoder.layers.2.self_attn.out_proj', 'model.decoder.layers.2.fc1', 'model.decoder.layers.2.fc2', 'model.decoder.layers.3.self_attn.q_proj', 'model.decoder.layers.3.self_attn.k_proj', 'model.decoder.layers.3.self_attn.v_proj', 'model.decoder.layers.3.self_attn.out_proj', 'model.decoder.layers.3.fc1', 'model.decoder.layers.3.fc2', 'model.decoder.layers.4.self_attn.q_proj', 'model.decoder.layers.4.self_attn.k_proj', 'model.decoder.layers.4.self_attn.v_proj', 'model.decoder.layers.4.self_attn.out_proj', 'model.decoder.layers.4.fc1', 'model.decoder.layers.4.fc2', 'model.decoder.layers.5.self_attn.q_proj', 'model.decoder.layers.5.self_attn.k_proj', 'model.decoder.layers.5.self_attn.v_proj', 'model.decoder.layers.5.self_attn.out_proj', 'model.decoder.layers.5.fc1', 'model.decoder.layers.5.fc2', 'model.decoder.layers.6.self_attn.q_proj', 'model.decoder.layers.6.self_attn.k_proj', 'model.decoder.layers.6.self_attn.v_proj', 'model.decoder.layers.6.self_attn.out_proj', 'model.decoder.layers.6.fc1', 'model.decoder.layers.6.fc2', 'model.decoder.layers.7.self_attn.q_proj', 'model.decoder.layers.7.self_attn.k_proj', 'model.decoder.layers.7.self_attn.v_proj', 'model.decoder.layers.7.self_attn.out_proj', 'model.decoder.layers.7.fc1', 'model.decoder.layers.7.fc2', 'model.decoder.layers.8.self_attn.q_proj', 'model.decoder.layers.8.self_attn.k_proj', 'model.decoder.layers.8.self_attn.v_proj', 'model.decoder.layers.8.self_attn.out_proj', 'model.decoder.layers.8.fc1', 'model.decoder.layers.8.fc2', 'model.decoder.layers.9.self_attn.q_proj', 'model.decoder.layers.9.self_attn.k_proj', 'model.decoder.layers.9.self_attn.v_proj', 'model.decoder.layers.9.self_attn.out_proj', 'model.decoder.layers.9.fc1', 'model.decoder.layers.9.fc2', 'model.decoder.layers.10.self_attn.q_proj', 'model.decoder.layers.10.self_attn.k_proj', 'model.decoder.layers.10.self_attn.v_proj', 'model.decoder.layers.10.self_attn.out_proj', 'model.decoder.layers.10.fc1', 'model.decoder.layers.10.fc2', 'model.decoder.layers.11.self_attn.q_proj', 'model.decoder.layers.11.self_attn.k_proj', 'model.decoder.layers.11.self_attn.v_proj', 'model.decoder.layers.11.self_attn.out_proj', 'model.decoder.layers.11.fc1', 'model.decoder.layers.11.fc2', 'model.decoder.layers.12.self_attn.q_proj', 'model.decoder.layers.12.self_attn.k_proj', 'model.decoder.layers.12.self_attn.v_proj', 'model.decoder.layers.12.self_attn.out_proj', 'model.decoder.layers.12.fc1', 'model.decoder.layers.12.fc2', 'model.decoder.layers.13.self_attn.q_proj', 'model.decoder.layers.13.self_attn.k_proj', 'model.decoder.layers.13.self_attn.v_proj', 'model.decoder.layers.13.self_attn.out_proj', 'model.decoder.layers.13.fc1', 'model.decoder.layers.13.fc2', 'model.decoder.layers.14.self_attn.q_proj', 'model.decoder.layers.14.self_attn.k_proj', 'model.decoder.layers.14.self_attn.v_proj', 'model.decoder.layers.14.self_attn.out_proj', 'model.decoder.layers.14.fc1', 'model.decoder.layers.14.fc2', 'model.decoder.layers.15.self_attn.q_proj', 'model.decoder.layers.15.self_attn.k_proj', 'model.decoder.layers.15.self_attn.v_proj', 'model.decoder.layers.15.self_attn.out_proj', 'model.decoder.layers.15.fc1', 'model.decoder.layers.15.fc2', 'model.decoder.layers.16.self_attn.q_proj', 'model.decoder.layers.16.self_attn.k_proj', 'model.decoder.layers.16.self_attn.v_proj', 'model.decoder.layers.16.self_attn.out_proj', 'model.decoder.layers.16.fc1', 'model.decoder.layers.16.fc2', 'model.decoder.layers.17.self_attn.q_proj', 'model.decoder.layers.17.self_attn.k_proj', 'model.decoder.layers.17.self_attn.v_proj', 'model.decoder.layers.17.self_attn.out_proj', 'model.decoder.layers.17.fc1', 'model.decoder.layers.17.fc2', 'model.decoder.layers.18.self_attn.q_proj', 'model.decoder.layers.18.self_attn.k_proj', 'model.decoder.layers.18.self_attn.v_proj', 'model.decoder.layers.18.self_attn.out_proj', 'model.decoder.layers.18.fc1', 'model.decoder.layers.18.fc2', 'model.decoder.layers.19.self_attn.q_proj', 'model.decoder.layers.19.self_attn.k_proj', 'model.decoder.layers.19.self_attn.v_proj', 'model.decoder.layers.19.self_attn.out_proj', 'model.decoder.layers.19.fc1', 'model.decoder.layers.19.fc2', 'model.decoder.layers.20.self_attn.q_proj', 'model.decoder.layers.20.self_attn.k_proj', 'model.decoder.layers.20.self_attn.v_proj', 'model.decoder.layers.20.self_attn.out_proj', 'model.decoder.layers.20.fc1', 'model.decoder.layers.20.fc2', 'model.decoder.layers.21.self_attn.q_proj', 'model.decoder.layers.21.self_attn.k_proj', 'model.decoder.layers.21.self_attn.v_proj', 'model.decoder.layers.21.self_attn.out_proj', 'model.decoder.layers.21.fc1', 'model.decoder.layers.21.fc2', 'model.decoder.layers.22.self_attn.q_proj', 'model.decoder.layers.22.self_attn.k_proj', 'model.decoder.layers.22.self_attn.v_proj', 'model.decoder.layers.22.self_attn.out_proj', 'model.decoder.layers.22.fc1', 'model.decoder.layers.22.fc2', 'model.decoder.layers.23.self_attn.q_proj', 'model.decoder.layers.23.self_attn.k_proj', 'model.decoder.layers.23.self_attn.v_proj', 'model.decoder.layers.23.self_attn.out_proj', 'model.decoder.layers.23.fc1', 'model.decoder.layers.23.fc2', 'lm_head'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 40/40 [00:24<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 14.84560\n",
      "model size: 104.38248 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# W8A8 quantization with salient weight AND activation protection\n",
    "w_n_bits = 8\n",
    "a_n_bits = 8\n",
    "\n",
    "# a) compute input features from calibration dataset\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "input_feats = get_calib_feat(model, tokenizer)\n",
    "\n",
    "# print out all keys in input_feats\n",
    "print(input_feats.keys())\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "model = quantize_opt_salient_weight_act_fp16(model, input_feats, w_n_bits=w_n_bits, a_n_bits=a_n_bits, act_quant=\"per_token\")\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "model_size = get_model_size(model, data_width=w_n_bits, group_size=128)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.5f}\")\n",
    "print(f\"model size: {model_size/MiB:.5f} MiB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
