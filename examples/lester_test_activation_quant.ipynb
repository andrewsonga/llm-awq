{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer):\n",
    "    testenc = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testenc[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    testenc = testenc.input_ids.to(model.device)\n",
    "    nsamples = 40\n",
    "    model = model.eval()\n",
    "\n",
    "    nlls = []\n",
    "    for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n",
    "        batch = testenc[:, (i * 2048) : ((i + 1) * 2048)].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            lm_logits = model(batch).logits\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "        shift_labels = testenc[:, (i * 2048) : ((i + 1) * 2048)][:, 1:]\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "        )\n",
    "        neg_log_likelihood = loss.float() * 2048\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model: nn.Module, data_width=16, group_size=-1):\n",
    "\n",
    "    if group_size != -1:\n",
    "        data_width += (16 + 4) / group_size\n",
    "\n",
    "    num_elements = 0\n",
    "    for param in model.parameters():\n",
    "        num_elements += param.numel()\n",
    "    return num_elements * data_width\n",
    "\n",
    "\n",
    "Byte = 8\n",
    "KiB = 1024 * Byte\n",
    "MiB = 1024 * KiB\n",
    "GiB = 1024 * MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sizheli/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cd414957074ecdba26fed37323678b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sizheli/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"facebook/opt-1.3b\"\n",
    "\n",
    "model_path = \"facebook/opt-13b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...:  32%|███▎      | 13/40 [01:25<02:59,  6.65s/it]"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "model_size = get_model_size(model, data_width=32, group_size=128)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "print(f\"model size: {model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from awq.quantize.quantizer import (\n",
    "#     real_quantize_model_weight,\n",
    "#     pseudo_quantize_model_weight,\n",
    "#     pseudo_quantize_tensor,\n",
    "# )\n",
    "\n",
    "# def quantize_opt_1(\n",
    "#     model,\n",
    "#     w_n_bits: int = 4,\n",
    "#     a_n_bits: int = 4,\n",
    "#     zero_point: bool = True,\n",
    "#     group_size: int = 128,\n",
    "# ):\n",
    "#     from transformers.models.opt.modeling_opt import (\n",
    "#         OPTAttention,\n",
    "#         OPTDecoderLayer,\n",
    "#     )\n",
    "\n",
    "#     for name, m in model.model.named_modules():\n",
    "#         if isinstance(m, OPTDecoderLayer):\n",
    "#             m.fc1.weight.data = pseudo_quantize_tensor(\n",
    "#                 m.fc1.weight.data,\n",
    "#                 n_bit=w_n_bits,\n",
    "#                 zero_point=zero_point,\n",
    "#                 q_group_size=group_size,\n",
    "#             )\n",
    "#             m.fc2.weight.data = pseudo_quantize_tensor(\n",
    "#                 m.fc2.weight.data,\n",
    "#                 n_bit=w_n_bits,\n",
    "#                 zero_point=zero_point,\n",
    "#                 q_group_size=group_size,\n",
    "#             )\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     \"zero_point\": True,  # by default True\n",
    "#     \"q_group_size\": 128,  # whether to use group quantization\n",
    "# }\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "# model = quantize_opt_1(model, w_n_bits=4, zero_point=True, group_size=128)\n",
    "# model.cuda()\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# model_perplexity = evaluate(model, tokenizer)\n",
    "# model_size = get_model_size(model, data_width=32, group_size=128)\n",
    "# print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "# print(f\"model size: {model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from awq.quantize.pre_quant import run_awq, apply_awq\n",
    "\n",
    "# q_config = {\n",
    "#     \"zero_point\": True,  # by default True\n",
    "#     \"q_group_size\": 128,  # whether to use group quantization\n",
    "# }\n",
    "\n",
    "# awq_results = run_awq(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     w_bit=4,\n",
    "#     q_config=q_config,\n",
    "#     n_samples=128,\n",
    "#     seqlen=512,\n",
    "# )\n",
    "\n",
    "# dump_awq = \"awq_results.pt\"\n",
    "# torch.save(awq_results, dump_awq)\n",
    "# print(\"AWQ results saved at\", dump_awq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_awq = \"awq_results.pt\"\n",
    "# torch.save(awq_results, dump_awq)\n",
    "# print(\"AWQ results saved at\", dump_awq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from awq.quantize.pre_quant import run_awq, apply_awq\n",
    "\n",
    "\n",
    "# load_awq = \"awq_results.pt\"\n",
    "# awq_results = torch.load(load_awq, map_location=\"cpu\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "# apply_awq(model, awq_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from awq.quantize.quantizer import (\n",
    "#     real_quantize_model_weight,\n",
    "#     pseudo_quantize_model_weight,\n",
    "# )\n",
    "\n",
    "# q_config = {\n",
    "#     \"zero_point\": True,  # by default True\n",
    "#     \"q_group_size\": 128,  # whether to use group quantization\n",
    "# }\n",
    "\n",
    "# pseudo_quantize_model_weight(model, w_bit=4, q_config=q_config)\n",
    "# model.cuda()\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# model_perplexity = evaluate(model, tokenizer)\n",
    "# model_size = get_model_size(model, data_width=32, group_size=128)\n",
    "# print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "# print(f\"model size: {model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from awq.quantize.quantizer import (\n",
    "#     real_quantize_model_weight,\n",
    "#     pseudo_quantize_model_weight,\n",
    "# )\n",
    "\n",
    "# q_config = {\n",
    "#     \"zero_point\": True,  # by default True\n",
    "#     \"q_group_size\": 128,  # whether to use group quantization\n",
    "# }\n",
    "\n",
    "# pseudo_quantize_model_weight(model, w_bit=4, q_config=q_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from awq.quantize.quantizer import pseudo_quantize_tensor\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_token_absmax(t, n_bits=8):\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_tensor_absmax(t, n_bits=8):\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        w_n_bits=4,\n",
    "        a_n_bits=16,\n",
    "        act_quant: Literal[\"per_token\", \"per_tensor\", \"none\"] = \"per_token\",\n",
    "        quantize_output: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randn(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.float16,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.zeros(\n",
    "                    (1, self.out_features), dtype=torch.float16, requires_grad=False\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "        if act_quant == \"per_token\":\n",
    "            self.act_quant_name = \"per_token\"\n",
    "            self.act_quant = partial(\n",
    "                quantize_activation_per_token_absmax, n_bits=a_n_bits\n",
    "            )\n",
    "        elif act_quant == \"per_tensor\":\n",
    "            self.act_quant_name = \"per_tensor\"\n",
    "            self.act_quant = partial(\n",
    "                quantize_activation_per_tensor_absmax, n_bits=a_n_bits\n",
    "            )\n",
    "        else:\n",
    "            self.act_quant_name = \"None\"\n",
    "            self.act_quant = lambda x: x\n",
    "\n",
    "        if quantize_output:\n",
    "            self.output_quant_name = self.act_quant_name\n",
    "            self.output_quant = self.act_quant\n",
    "        else:\n",
    "            self.output_quant_name = \"None\"\n",
    "            self.output_quant = lambda x: x\n",
    "\n",
    "        # self.act_quant = lambda x: x\n",
    "        # self.output_quant = lambda x: x\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super(QuantizedLinear, self).to(*args, **kwargs)\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        q_x = self.act_quant(x)\n",
    "        q_x = x\n",
    "        y = F.linear(q_x, self.weight, self.bias)\n",
    "        q_y = self.output_quant(y)\n",
    "        return q_y\n",
    "\n",
    "    @classmethod\n",
    "    def from_linear(\n",
    "        cls,\n",
    "        linear: nn.Linear,\n",
    "        w_n_bits: int = 4,\n",
    "        a_n_bits: int = 4,\n",
    "        zero_point: bool = True,\n",
    "        group_size: int = 128,\n",
    "        act_quant: Literal[\"per_token\", \"per_tensor\", \"none\"] = \"per_token\",\n",
    "    ):\n",
    "\n",
    "        awq_linear = cls(\n",
    "            linear.in_features,\n",
    "            linear.out_features,\n",
    "            bias=linear.bias is not None,\n",
    "            w_n_bits=w_n_bits,\n",
    "            a_n_bits=a_n_bits,\n",
    "            act_quant=act_quant,\n",
    "        )\n",
    "\n",
    "        awq_linear.weight.data = pseudo_quantize_tensor(\n",
    "            w=linear.weight.data,\n",
    "            n_bit=w_n_bits,\n",
    "            zero_point=zero_point,\n",
    "            q_group_size=group_size,\n",
    "        )\n",
    "\n",
    "        if linear.bias is not None:\n",
    "            awq_linear.bias.data = linear.bias.data\n",
    "\n",
    "        return awq_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_opt(\n",
    "    model,\n",
    "    w_n_bits: int = 4,\n",
    "    a_n_bits: int = 4,\n",
    "    zero_point: bool = True,\n",
    "    group_size: int = 128,\n",
    "    act_quant: Literal[\"per_token\", \"per_tensor\", \"none\"] = \"per_token\",\n",
    "):\n",
    "    from transformers.models.opt.modeling_opt import (\n",
    "        OPTAttention,\n",
    "        OPTDecoderLayer,\n",
    "    )\n",
    "\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, OPTDecoderLayer):\n",
    "            m.fc1 = QuantizedLinear.from_linear(\n",
    "                m.fc1,\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "            )\n",
    "            m.fc2 = QuantizedLinear.from_linear(\n",
    "                m.fc2,\n",
    "                w_n_bits=w_n_bits,\n",
    "                a_n_bits=a_n_bits,\n",
    "                zero_point=zero_point,\n",
    "                group_size=group_size,\n",
    "                act_quant=act_quant,\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_n_bits = 8\n",
    "a_n_bits = 8\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "# apply the AWQ results\n",
    "# apply_awq(model, awq_results)\n",
    "\n",
    "model = quantize_opt(\n",
    "    \n",
    "\n",
    "    model, w_n_bits=w_n_bits, a_n_bits=a_n_bits, act_quant=\"per_tensor\"\n",
    ")\n",
    "model.cuda()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "model_size = get_model_size(model, data_width=w_n_bits, group_size=128)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "print(f\"model size: {model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
