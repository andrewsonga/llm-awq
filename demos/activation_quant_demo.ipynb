{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, nsamples: int = 40):\n",
    "    testenc = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testenc[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    testenc = testenc.input_ids.to(model.device)\n",
    "    model = model.eval()\n",
    "\n",
    "    nlls = []\n",
    "    for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n",
    "        batch = testenc[:, (i * 2048) : ((i + 1) * 2048)].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            lm_logits = model(batch).logits\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "        shift_labels = testenc[:, (i * 2048) : ((i + 1) * 2048)][:, 1:]\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fct(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "        )\n",
    "        neg_log_likelihood = loss.float() * 2048\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model: nn.Module, data_width=16, group_size=-1):\n",
    "\n",
    "    if group_size != -1:\n",
    "        data_width += (16 + 4) / group_size\n",
    "\n",
    "    num_elements = 0\n",
    "    for param in model.parameters():\n",
    "        num_elements += param.numel()\n",
    "    return num_elements * data_width\n",
    "\n",
    "\n",
    "Byte = 8\n",
    "KiB = 1024 * Byte\n",
    "MiB = 1024 * KiB\n",
    "GiB = 1024 * MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sizheli/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"facebook/opt-1.3b\"\n",
    "\n",
    "# model_path = \"facebook/opt-2.7b\"\n",
    "\n",
    "model_path = \"facebook/opt-6.7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb099034152463481a17407bb9d26a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sizheli/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "original_model_n_bits = 16\n",
    "torch_dtype = torch.float16 if original_model_n_bits == 16 else torch.float32\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch_dtype, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Evaluate the model\n",
    "# model_perplexity = evaluate(model, tokenizer)\n",
    "# model_size = get_model_size(model, data_width=original_model_n_bits, group_size=128)\n",
    "\n",
    "# ### Print the results\n",
    "# print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "# print(f\"model size: {model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq.quantize.pre_quant_new import run_awq, apply_awq\n",
    "from typing import Literal\n",
    "\n",
    "ActQuantType = Literal[\"per_token\", \"per_tensor\", \"none\", \"per_channel\"]\n",
    "\n",
    "q_config = {\n",
    "    \"zero_point\": True,  # by default True\n",
    "    \"q_group_size\": 128,  # whether to use group quantization\n",
    "    \"w_n_bits\": 4,\n",
    "    \"a_n_bits\": 4,\n",
    "    \"act_quant\": \"none\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch_dtype, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "awq_results = run_awq(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    w_bit=q_config[\"w_n_bits\"],\n",
    "    q_config=q_config,\n",
    "    n_samples=128,\n",
    "    seqlen=512,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_awq = \"awq_results.pt\"\n",
    "# torch.save(awq_results, dump_awq)\n",
    "# print(\"AWQ results saved at\", dump_awq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### load awq\n",
    "# load_awq = \"awq_results.pt\"\n",
    "# awq_results = torch.load(load_awq, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.opt.modeling_opt import OPTDecoderLayer\n",
    "from awq.quantize.fake_quant_new import quantize_opt_model\n",
    "\n",
    "model_path = \"facebook/opt-2.7b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch_dtype, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# apply the AWQ results\n",
    "# apply_awq(model, awq_results)\n",
    "\n",
    "\n",
    "model = quantize_opt_model(\n",
    "    model,\n",
    "    w_n_bits=q_config[\"w_n_bits\"],\n",
    "    a_n_bits=q_config[\"a_n_bits\"],\n",
    "    # act_quant=q_config[\"act_quant\"],\n",
    "    act_quant=\"per_token\",\n",
    "    group_size=q_config[\"q_group_size\"],\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 2560, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)\n",
       "      (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): QuantizedLinear()\n",
       "          (fc2): QuantizedLinear()\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|██████████| 100/100 [00:31<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 18856.74\n",
      "model size: 482.32 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer, nsamples=100)\n",
    "model_size = get_model_size(\n",
    "    model, data_width=q_config[\"w_n_bits\"], group_size=q_config[\"q_group_size\"]\n",
    ")\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "print(f\"model size: {model_size/MiB:.2f} MiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq.quantize.pre_quant_new import get_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in get_blocks(model):\n",
    "    # layer.fc1\n",
    "    # layer.fc2\n",
    "    # print(\"hi\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "x = deepcopy(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.fc1.weight.data[:] = 1.0\n",
    "# x.fc1.weight.data\n",
    "\n",
    "layer.fc1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(layer, OPTDecoderLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
